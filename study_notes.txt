✅ Project: Build LLM from scratch (Step-by-Step)

Progress:
- ✅ Tokenization:
  Text converted to tokens (words to numbers)

- ✅ Embedding Layer:
  Token IDs converted to vectors (random numbers initially)

- ✅ Attention Mechanism:
  - Generate Q (Query), K (Key), V (Value)
  - Calculate attention scores: Q @ K.T
  - Scale attention scores to prevent exploding values
  - Softmax to get attention weights
  - Multiply attention weights with V to get context vectors

- ✅ Feedforward Neural Network:
  - Context vectors passed through two layers (ReLU activation)
  - Refines output before final prediction

- ✅ Output Layer:
  - Feedforward output @ output weights = logits
  - Softmax to get probabilities
  - np.argmax to predict final words

- ✅ Multi-sentence dataset:
  - Using external file `texts.txt` for multiple sentences

- ✅ Model saving:
  - Final model weights saved in `models/` directory
  - vocab.pkl saved (for token-to-word mapping)

- ✅ Checkpoint saving:
  - Auto-saves model every 100 epochs in `models/checkpoint_epoch_X/`
  - Easy to load and continue training later

- ✅ Auto-resume training:
  - Automatically resumes from the latest checkpoint without manual input

- ✅ Loss curve graph:
  - Visualizes training progress
  - Saved at `models/loss_curve.png`

Next step:
➡️ Temperature sampling for generation
- Make output more creative and less repetitive
- Control randomness during text generation

Optional future improvements:
- Batch processing for faster training
- Larger dataset (books, Wikipedia)
- Multi-layer attention (closer to GPT models)
- Save loss history as CSV
- Generate text from checkpoints

Status:
✅ Clean and professional code
✅ Fully working mini-LLM from scratch
✅ Understood each step in training loop
✅ Good sleep after productive learning! 🌙

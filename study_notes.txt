âœ… Project: Build LLM from scratch (Step-by-Step)

Progress:
- âœ… Tokenization:
  Text converted to tokens (words to numbers)

- âœ… Embedding Layer:
  Token IDs converted to vectors (random numbers initially)

- âœ… Attention Mechanism:
  - Generate Q (Query), K (Key), V (Value)
  - Calculate attention scores: Q @ K.T
  - Scale attention scores to prevent exploding values
  - Softmax to get attention weights
  - Multiply attention weights with V to get context vectors

- âœ… Feedforward Neural Network:
  - Context vectors passed through two layers (ReLU activation)
  - Refines output before final prediction

- âœ… Output Layer:
  - Feedforward output @ output weights = logits
  - Softmax to get probabilities
  - np.argmax to predict final words

- âœ… Multi-sentence dataset:
  - Using external file `texts.txt` for multiple sentences

- âœ… Model saving:
  - Final model weights saved in `models/` directory
  - vocab.pkl saved (for token-to-word mapping)

- âœ… Checkpoint saving:
  - Auto-saves model every 100 epochs in `models/checkpoint_epoch_X/`
  - Easy to load and continue training later

- âœ… Auto-resume training:
  - Automatically resumes from the latest checkpoint without manual input

- âœ… Loss curve graph:
  - Visualizes training progress
  - Saved at `models/loss_curve.png`

Next step:
â¡ï¸ Temperature sampling for generation
- Make output more creative and less repetitive
- Control randomness during text generation

Optional future improvements:
- Batch processing for faster training
- Larger dataset (books, Wikipedia)
- Multi-layer attention (closer to GPT models)
- Save loss history as CSV
- Generate text from checkpoints

Status:
âœ… Clean and professional code
âœ… Fully working mini-LLM from scratch
âœ… Understood each step in training loop
âœ… Good sleep after productive learning! ğŸŒ™

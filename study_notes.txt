ðŸ§© LLM from Scratch â€” Final Study Notes âœ…

âœ… Step 1: Tokenizer
- Break text into tokens (words).
- Assign each unique token a number (vocab).
- Example: "Hello how are you hello"
â†’ Vocab: {Hello: 0, how: 1, are: 2, you: 3, hello: 4}
â†’ Encoded: [0, 1, 2, 3, 4]

âœ… Step 2: Embedding
- Convert tokens into vectors (arrays of numbers) with random values.
- Each word gets a vector like:
  'Hello' â†’ [0.12, 0.53, 0.92, ...]
- These vectors represent "meaning in math space."

âœ… Step 3: Attention Mechanism
- Brain of the model: decides which words are important.
- Create three versions of vectors:
  - Q = Query (what we are asking)
  - K = Key (what knowledge is stored)
  - V = Value (what meaning to use)
  
- Compute focus (attention scores):
  `Q @ K.T` â†’ Compare queries to keys.

- Normalize with softmax:
  â†’ Convert scores to percentages (attention weights).

- Use attention weights:
  `attention_weights @ V`
  â†’ Blend the important meanings together (context vectors).

âœ… Step 4: Feedforward Layer
- Pass context vectors through a small neural network for deeper thinking.
- Layer 1: Multiply by weights, add bias, apply ReLU activation.
- Layer 2: Multiply by second weights, add bias.
- Output: refined final word vectors, full of meaning.

âœ… Step 5: Output Layer (Prediction)
- Map final vectors to vocabulary size using random weights.
- Apply softmax to get probabilities for each word.
- Pick the word with the highest probability: `np.argmax()`

ðŸŽ‰ Result: Model predicts a word from vocabulary!

âœ… Notes:
- Our model is not trained yet, so it repeats one word (like 'Hello') â€” this is expected!
- If we train it, the random weights will adjust, and it will start making real predictions.

ðŸ§  Python tips:
- `.shape`: Get rows and columns of matrix.
- `@`: Matrix multiplication (transform vectors).
- `np.exp()`: Exponential for softmax.
- `np.argmax()`: Pick highest probability.

ðŸš€ Completion:
Tokenizer âœ…
Embedding âœ…
Attention âœ…
Feedforward âœ…
Output Prediction âœ…
